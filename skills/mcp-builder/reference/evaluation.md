# MCP 服务器评估指南

## 概述

本文档提供为 MCP 服务器创建全面评估的指导。评估测试 LLM 能否仅使用提供的工具有效地使用您的 MCP 服务器回答真实、复杂的问题。

---

## 快速参考

### 评估要求
- 创建 10 个人类可读的问题
- 问题必须是只读、独立、非破坏性的
- 每个问题需要多次工具调用（可能数十次）
- 答案必须是单一、可验证的值
- 答案必须稳定（不会随时间改变）

### 输出格式
```xml
<evaluation>
   <qa_pair>
      <question>您的问题在这里</question>
      <answer>单一可验证的答案</answer>
   </qa_pair>
</evaluation>
```

---

## 评估目的

MCP 服务器质量的衡量标准不是服务器实现工具的好坏或全面性，而是这些实现（输入/输出模式、文档字符串/描述、功能）能多好地使没有其他上下文且仅能访问 MCP 服务器的 LLM 回答真实且困难的问题。

## 评估概述

创建 10 个人类可读的问题，这些问题仅需要只读、独立、非破坏性和幂等操作来回答。每个问题应该：
- 真实
- 清晰简洁
- 明确无歧义
- 复杂，可能需要数十次工具调用或步骤
- 可用您预先确定的单一、可验证值来回答

## 问题指南

### 核心要求

1. **问题必须独立**
   - 每个问题不应依赖于任何其他问题的答案
   - 不应假设处理另一个问题时进行了先前的写操作

2. **问题必须仅需要非破坏性和幂等的工具使用**
   - 不应指示或要求修改状态来获得正确答案

3. **问题必须真实、清晰、简洁且复杂**
   - 必须要求另一个 LLM 使用多个（可能数十个）工具或步骤来回答

### 复杂性和深度

4. **问题必须需要深度探索**
   - 考虑需要多个子问题和顺序工具调用的多跳问题
   - 每个步骤应受益于在前面问题中发现的信息

5. **问题可能需要大量分页**
   - 可能需要翻阅多页结果
   - 可能需要查询旧数据（1-2 年前）以找到小众信息
   - 问题必须困难

6. **问题必须需要深度理解**
   - 而不是表面知识
   - 可以将复杂想法作为需要证据的真/假问题
   - 可以使用选择题格式，LLM 必须搜索不同假设

7. **问题不能通过直接关键词搜索解决**
   - 不要包含目标内容中的特定关键词
   - 使用同义词、相关概念或释义
   - 需要多次搜索、分析多个相关项目、提取上下文，然后推导答案

### 工具测试

8. **问题应压力测试工具返回值**
   - 可能触发工具返回大型 JSON 对象或列表，使 LLM 不堪重负
   - 应要求理解多种数据模态：
     - ID 和名称
     - 时间戳和日期时间（月、日、年、秒）
     - 文件 ID、名称、扩展名和 MIME 类型
     - URL、GID 等
   - 应探测工具返回所有有用数据形式的能力

9. **问题应大多反映真实的人类用例**
   - 由 LLM 辅助的人类会关心的信息检索任务类型

10. **问题可能需要数十次工具调用**
    - 这挑战上下文有限的 LLM
    - 鼓励 MCP 服务器工具减少返回的信息

11. **包含模糊问题**
    - 可能是模糊的或需要对调用哪些工具做出困难决定
    - 迫使 LLM 可能犯错或误解
    - 确保尽管模糊，仍有单一可验证答案

### 稳定性

12. **问题必须设计为答案不会改变**
    - 不要问依赖于动态"当前状态"的问题
    - 例如，不要计数：
      - 帖子的反应数
      - 线程的回复数
      - 频道的成员数

13. **不要让 MCP 服务器限制您创建的问题类型**
    - 创建具有挑战性和复杂性的问题
    - 有些可能无法用可用的 MCP 服务器工具解决
    - 问题可能需要特定的输出格式（日期时间 vs. 纪元时间，JSON vs. MARKDOWN）
    - 问题可能需要数十次工具调用才能完成

## 答案指南

### 验证

1. **答案必须可通过直接字符串比较验证**
   - 如果答案可以用多种格式重写，在问题中明确指定输出格式
   - 示例："使用 YYYY/MM/DD。"、"回答 True 或 False。"、"回答 A、B、C 或 D，不要其他内容。"
   - 答案应该是单一可验证值，例如：
     - 用户 ID、用户名、显示名、名字、姓氏
     - 频道 ID、频道名
     - 消息 ID、字符串
     - URL、标题
     - 数量
     - 时间戳、日期时间
     - 布尔值（用于真/假问题）
     - 电子邮件地址、电话号码
     - 文件 ID、文件名、文件扩展名
     - 选择题答案
   - 答案不需要特殊格式或复杂的结构化输出
   - 答案将使用直接字符串比较验证

### 可读性

2. **答案通常应优先使用人类可读格式**
   - 示例：名称、名字、姓氏、日期时间、文件名、消息字符串、URL、是/否、真/假、a/b/c/d
   - 而不是不透明的 ID（尽管 ID 也可接受）
   - 绝大多数答案应该是人类可读的

### 稳定性

3. **答案必须稳定/静止**
   - 查看旧内容（例如，已结束的对话、已启动的项目、已回答的问题）
   - 基于"已关闭"的概念创建问题，这些概念将始终返回相同答案
   - 问题可能要求考虑固定时间窗口以隔离非静止答案
   - 依赖不太可能改变的上下文
   - 示例：如果查找论文名称，要足够具体，这样答案不会与后来发表的论文混淆

4. **答案必须清晰明确**
   - 问题必须设计为有单一、清晰的答案
   - 答案可以通过使用 MCP 服务器工具得出

### 多样性

5. **答案必须多样**
   - 答案应该是多种模态和格式的单一可验证值
   - 用户概念：用户 ID、用户名、显示名、名字、姓氏、电子邮件地址、电话号码
   - 频道概念：频道 ID、频道名、频道主题
   - 消息概念：消息 ID、消息字符串、时间戳、月、日、年

6. **答案不能是复杂结构**
   - 不是值列表
   - 不是复杂对象
   - 不是 ID 或字符串列表
   - 不是自然语言文本
   - 除非答案可以直接使用直接字符串比较验证
   - 并且可以实际重现
   - LLM 不太可能以任何其他顺序或格式返回相同列表

## 评估流程

### 步骤 1：文档检查

阅读目标 API 的文档以了解：
- 可用端点和功能
- 如果存在歧义，从网上获取额外信息
- 尽可能并行化此步骤
- 确保每个子代理只检查来自文件系统或网络的文档

### 步骤 2：工具检查

列出 MCP 服务器中可用的工具：
- 直接检查 MCP 服务器
- 了解输入/输出模式、文档字符串和描述
- 在此阶段不调用工具本身

### 步骤 3：建立理解

重复步骤 1 和 2 直到您有良好的理解：
- 多次迭代
- 思考您想要创建的任务类型
- 完善您的理解
- 在任何阶段都不应读取 MCP 服务器实现本身的代码
- 使用您的直觉和理解创建合理、真实但非常具有挑战性的任务

### 步骤 4：只读内容检查

在理解 API 和工具后，使用 MCP 服务器工具：
- 仅使用只读和非破坏性操作检查内容
- 目标：识别用于创建真实问题的特定内容（例如，用户、频道、消息、项目、任务）
- 不应调用任何修改状态的工具
- 不会读取 MCP 服务器实现本身的代码
- 通过单独的子代理进行独立探索来并行化此步骤
- 确保每个子代理只执行只读、非破坏性和幂等操作
- 注意：某些工具可能返回大量数据，这会导致您耗尽上下文
- 进行增量、小规模和有针对性的工具调用进行探索
- 在所有工具调用请求中，使用 `limit` 参数限制结果（<10）
- 使用分页

### 步骤 5：任务生成

检查内容后，创建 10 个人类可读的问题：
- LLM 应该能够使用 MCP 服务器回答这些问题
- 遵循上述所有问题和答案指南

## 输出格式

每个 QA 对由一个问题和一个答案组成。输出应该是具有以下结构的 XML 文件：

```xml
<evaluation>
   <qa_pair>
      <question>查找 2024 年第二季度创建的完成任务最多的项目。项目名称是什么？</question>
      <answer>网站重设计</answer>
   </qa_pair>
   <qa_pair>
      <question>搜索 2024 年 3 月关闭的标记为"bug"的问题。哪个用户关闭的问题最多？提供他们的用户名。</question>
      <answer>sarah_dev</answer>
   </qa_pair>
   <qa_pair>
      <question>查找在 2024 年 1 月 1 日到 1 月 31 日之间合并的修改了 /api 目录文件的拉取请求。有多少不同的贡献者参与了这些 PR？</question>
      <answer>7</answer>
   </qa_pair>
   <qa_pair>
      <question>查找 2023 年之前创建的星标最多的仓库。仓库名称是什么？</question>
      <answer>data-pipeline</answer>
   </qa_pair>
</evaluation>
```

## 评估示例

### 好的问题

**示例 1：需要深度探索的多跳问题（GitHub MCP）**
```xml
<qa_pair>
   <question>查找 2023 年第三季度归档的、之前是组织中分叉最多的项目的仓库。该仓库使用的主要编程语言是什么？</question>
   <answer>Python</answer>
</qa_pair>
```

这个问题好因为：
- 需要多次搜索来查找归档的仓库
- 需要识别归档前分叉最多的仓库
- 需要检查仓库详情以获取语言
- 答案是简单、可验证的值
- 基于不会改变的历史（已关闭）数据

**示例 2：需要理解上下文而非关键词匹配（项目管理 MCP）**
```xml
<qa_pair>
   <question>找到 2023 年底完成的专注于改善客户入职的计划。项目负责人在完成后创建了回顾文档。当时负责人的职位是什么？</question>
   <answer>产品经理</answer>
</qa_pair>
```

这个问题好因为：
- 不使用特定项目名称（"专注于改善客户入职的计划"）
- 需要从特定时间范围查找已完成的项目
- 需要识别项目负责人及其角色
- 需要从回顾文档理解上下文
- 答案是人类可读且稳定的
- 基于已完成的工作（不会改变）

**示例 3：需要多步骤的复杂聚合（问题跟踪器 MCP）**
```xml
<qa_pair>
   <question>在 2024 年 1 月报告的所有标记为关键优先级的 bug 中，哪个被分配者在 48 小时内解决了最高百分比的分配 bug？提供被分配者的用户名。</question>
   <answer>alex_eng</answer>
</qa_pair>
```

这个问题好因为：
- 需要按日期、优先级和状态过滤 bug
- 需要按被分配者分组并计算解决率
- 需要理解时间戳以确定 48 小时窗口
- 测试分页（可能有很多 bug 要处理）
- 答案是单一用户名
- 基于特定时间段的历史数据

**示例 4：需要跨多种数据类型综合（CRM MCP）**
```xml
<qa_pair>
   <question>查找 2023 年第四季度从入门级升级到企业级且年度合同价值最高的账户。该账户所在的行业是什么？</question>
   <answer>医疗保健</answer>
</qa_pair>
```

这个问题好因为：
- 需要理解订阅级别变化
- 需要识别特定时间范围内的升级事件
- 需要比较合同价值
- 必须访问账户行业信息
- 答案简单且可验证
- 基于已完成的历史交易

### 差的问题

**示例 1：答案随时间变化**
```xml
<qa_pair>
   <question>当前分配给工程团队的未解决问题有多少？</question>
   <answer>47</answer>
</qa_pair>
```

这个问题差因为：
- 答案会随着问题的创建、关闭或重新分配而改变
- 不基于稳定/静止的数据
- 依赖于动态的"当前状态"

**示例 2：关键词搜索太简单**
```xml
<qa_pair>
   <question>查找标题为"添加身份验证功能"的拉取请求，告诉我谁创建的。</question>
   <answer>developer123</answer>
</qa_pair>
```

这个问题差因为：
- 可以通过直接搜索精确标题的关键词解决
- 不需要深度探索或理解
- 不需要综合或分析

**示例 3：答案格式模糊**
```xml
<qa_pair>
   <question>列出所有以 Python 为主要语言的仓库。</question>
   <answer>repo1, repo2, repo3, data-pipeline, ml-tools</answer>
</qa_pair>
```

这个问题差因为：
- 答案是可以任何顺序返回的列表
- 难以通过直接字符串比较验证
- LLM 可能格式不同（JSON 数组、逗号分隔、换行分隔）
- 最好问特定的聚合（计数）或最高级（星标最多）

## 验证流程

创建评估后：

1. **检查 XML 文件**以了解模式
2. **加载每个任务指令**并使用 MCP 服务器和工具并行识别正确答案，通过自己尝试解决任务
3. **标记任何操作**需要写入或破坏性操作的
4. **累积所有正确答案**并替换文档中的任何错误答案
5. **删除任何 `<qa_pair>`**需要写入或破坏性操作的

记住并行化解决任务以避免耗尽上下文，然后在最后累积所有答案并对文件进行更改。

## 创建高质量评估的技巧

1. **在生成任务前努力思考并提前规划**
2. **在有机会时并行化**以加快流程并管理上下文
3. **专注于真实用例**人类实际想要完成的
4. **创建具有挑战性的问题**测试 MCP 服务器功能的极限
5. **确保稳定性**使用历史数据和已关闭的概念
6. **验证答案**使用 MCP 服务器工具自己解决问题
7. **根据流程中学到的内容迭代和完善**

---

# 运行评估

创建评估文件后，您可以使用提供的评估工具来测试您的 MCP 服务器。

## 设置

1. **安装依赖**

   ```bash
   pip install -r scripts/requirements.txt
   ```

   或手动安装：
   ```bash
   pip install anthropic mcp
   ```

2. **设置 API 密钥**

   ```bash
   export ANTHROPIC_API_KEY=your_api_key_here
   ```

## 评估文件格式

评估文件使用带有 `<qa_pair>` 元素的 XML 格式：

```xml
<evaluation>
   <qa_pair>
      <question>查找 2024 年第二季度创建的完成任务最多的项目。项目名称是什么？</question>
      <answer>网站重设计</answer>
   </qa_pair>
   <qa_pair>
      <question>搜索 2024 年 3 月关闭的标记为"bug"的问题。哪个用户关闭的问题最多？提供他们的用户名。</question>
      <answer>sarah_dev</answer>
   </qa_pair>
</evaluation>
```

## 运行评估

评估脚本（`scripts/evaluation.py`）支持三种传输类型：

**重要：**
- **stdio 传输**：评估脚本自动为您启动和管理 MCP 服务器进程。不要手动运行服务器。
- **sse/http 传输**：您必须在运行评估之前单独启动 MCP 服务器。脚本连接到指定 URL 上已运行的服务器。

### 1. 本地 STDIO 服务器

用于本地运行的 MCP 服务器（脚本自动启动服务器）：

```bash
python scripts/evaluation.py \
  -t stdio \
  -c python \
  -a my_mcp_server.py \
  evaluation.xml
```

带环境变量：
```bash
python scripts/evaluation.py \
  -t stdio \
  -c python \
  -a my_mcp_server.py \
  -e API_KEY=abc123 \
  -e DEBUG=true \
  evaluation.xml
```

### 2. 服务器发送事件（SSE）

用于基于 SSE 的 MCP 服务器（您必须先启动服务器）：

```bash
python scripts/evaluation.py \
  -t sse \
  -u https://example.com/mcp \
  -H "Authorization: Bearer token123" \
  -H "X-Custom-Header: value" \
  evaluation.xml
```

### 3. HTTP（流式 HTTP）

用于基于 HTTP 的 MCP 服务器（您必须先启动服务器）：

```bash
python scripts/evaluation.py \
  -t http \
  -u https://example.com/mcp \
  -H "Authorization: Bearer token123" \
  evaluation.xml
```

## 命令行选项

```
usage: evaluation.py [-h] [-t {stdio,sse,http}] [-m MODEL] [-c COMMAND]
                     [-a ARGS [ARGS ...]] [-e ENV [ENV ...]] [-u URL]
                     [-H HEADERS [HEADERS ...]] [-o OUTPUT]
                     eval_file

位置参数:
  eval_file             评估 XML 文件的路径

可选参数:
  -h, --help            显示帮助信息
  -t, --transport       传输类型：stdio、sse 或 http（默认：stdio）
  -m, --model           要使用的 Claude 模型（默认：claude-3-7-sonnet-20250219）
  -o, --output          报告输出文件（默认：打印到标准输出）

stdio 选项:
  -c, --command         运行 MCP 服务器的命令（例如 python、node）
  -a, --args            命令的参数（例如 server.py）
  -e, --env             KEY=VALUE 格式的环境变量

sse/http 选项:
  -u, --url             MCP 服务器 URL
  -H, --header          'Key: Value' 格式的 HTTP 头
```

## 输出

评估脚本生成详细报告，包括：

- **摘要统计**：
  - 准确率（正确/总数）
  - 平均任务时长
  - 每个任务的平均工具调用次数
  - 总工具调用次数

- **每个任务的结果**：
  - 提示和预期响应
  - 代理的实际响应
  - 答案是否正确（✅/❌）
  - 时长和工具调用详情
  - 代理方法的摘要
  - 代理对工具的反馈

### 保存报告到文件

```bash
python scripts/evaluation.py \
  -t stdio \
  -c python \
  -a my_server.py \
  -o evaluation_report.md \
  evaluation.xml
```

## 完整示例工作流程

以下是创建和运行评估的完整示例：

1. **创建评估文件**（`my_evaluation.xml`）：

```xml
<evaluation>
   <qa_pair>
      <question>查找 2024 年 1 月创建问题最多的用户。他们的用户名是什么？</question>
      <answer>alice_developer</answer>
   </qa_pair>
   <qa_pair>
      <question>在 2024 年第一季度合并的所有拉取请求中，哪个仓库数量最多？提供仓库名称。</question>
      <answer>backend-api</answer>
   </qa_pair>
   <qa_pair>
      <question>查找 2023 年 12 月完成的、从开始到结束时间最长的项目。花了多少天？</question>
      <answer>127</answer>
   </qa_pair>
</evaluation>
```

2. **安装依赖**：

```bash
pip install -r scripts/requirements.txt
export ANTHROPIC_API_KEY=your_api_key
```

3. **运行评估**：

```bash
python scripts/evaluation.py \
  -t stdio \
  -c python \
  -a github_mcp_server.py \
  -e GITHUB_TOKEN=ghp_xxx \
  -o github_eval_report.md \
  my_evaluation.xml
```

4. **查看报告** `github_eval_report.md` 以：
   - 查看哪些问题通过/失败
   - 阅读代理对您工具的反馈
   - 识别需要改进的领域
   - 迭代您的 MCP 服务器设计

## 故障排除

### 连接错误

如果遇到连接错误：
- **STDIO**：验证命令和参数是否正确
- **SSE/HTTP**：检查 URL 是否可访问且头信息是否正确
- 确保任何所需的 API 密钥已在环境变量或头信息中设置

### 低准确率

如果许多评估失败：
- 查看每个任务的代理反馈
- 检查工具描述是否清晰全面
- 验证输入参数是否有良好的文档
- 考虑工具是否返回了太多或太少的数据
- 确保错误消息是可操作的

### 超时问题

如果任务超时：
- 使用更强大的模型（例如 `claude-3-7-sonnet-20250219`）
- 检查工具是否返回了太多数据
- 验证分页是否正常工作
- 考虑简化复杂问题
